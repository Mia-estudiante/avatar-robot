{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from models.gdino import GDINO\n",
    "from models.llama import Llama\n",
    "from visualizer import Visualizer\n",
    "\n",
    "from sam2.build_sam import build_sam2_object_tracker\n",
    "torch_dtype=torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26eb66ca244d0f0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set SAM2 Configuration\n",
    "NUM_OBJECTS = 1\n",
    "SAM_CHECKPOINT_FILEPATH = \"./checkpoints/sam2.1_hiera_tiny.pt\"\n",
    "SAM_CONFIG_FILEPATH = \"./configs/samurai/sam2.1_hiera_t.yaml\"\n",
    "# SAM_CONFIG_FILEPATH = \"./configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b3205",
   "metadata": {},
   "source": [
    "parameter를 이용해 hugging face로부터 모델을 불러올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d909a1eff131e1e1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sam = build_sam2_object_tracker(num_objects=NUM_OBJECTS,\n",
    "                                config_file=SAM_CONFIG_FILEPATH,\n",
    "                                ckpt_path=SAM_CHECKPOINT_FILEPATH,\n",
    "                                device=DEVICE,\n",
    "                                verbose=False\n",
    "                                )\n",
    "gdino = GDINO()\n",
    "gdino.build_model()\n",
    "\n",
    "llama = Llama()\n",
    "llama.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b73b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Video Stream\n",
    "# video_stream = cv2.VideoCapture(VIDEO_STREAM)\n",
    "video_stream = cv2.VideoCapture(0)\n",
    "\n",
    "video_height = int(video_stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "video_width = int(video_stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "# For real-time visualization\n",
    "visualizer = Visualizer(video_width=video_width, video_height=video_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14f4295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(images_pil, texts_prompt, box_threshold, text_threshold):\n",
    "    gdino_results = gdino.predict(images_pil, [texts_prompt], box_threshold, text_threshold)\n",
    "    sam_boxes = []\n",
    "    sam_indices = []\n",
    "    for idx, result in enumerate(gdino_results):\n",
    "        result = {k: (v.cpu().numpy() if hasattr(v, \"numpy\") else v) for k, v in result.items()}\n",
    "        processed_result = {\n",
    "            **result,\n",
    "            \"masks\": [],\n",
    "            \"mask_scores\": [],\n",
    "        }\n",
    "\n",
    "        sam_boxes.append(processed_result[\"boxes\"])\n",
    "        sam_indices.append(idx)\n",
    "\n",
    "    return sam_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293c841",
   "metadata": {},
   "source": [
    "아래는 query의 예시입니다.   \n",
    "query = \"I am dehydrated\"   \n",
    "query = \"I am thirsty\"   \n",
    "query = \"I want to read\"   \n",
    "query = \"I am bored\"   \n",
    "query = \"I need a tool for writing\"   \n",
    "query = \"I have to write something down\"   \n",
    "query = \"I have to call him\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e8aa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are you looking for? bottle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/home/MR/lang-sam/visualizer.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(mask, device='cpu')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     first_frame = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     sam_out = \u001b[43msam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrack_all_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m ret, frame = video_stream.read()\n\u001b[32m     27\u001b[39m visualizer.add_frame(frame=frame, mask=sam_out[\u001b[33m'\u001b[39m\u001b[33mpred_masks\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang-sam2/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MR/lang-sam/sam2/sam2_object_tracker.py:1069\u001b[39m, in \u001b[36mSAM2ObjectTracker.track_all_objects\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m   1067\u001b[39m \u001b[38;5;66;03m# Prepare image for inference\u001b[39;00m\n\u001b[32m   1068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, np.ndarray):\n\u001b[32m-> \u001b[39m\u001b[32m1069\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1071\u001b[39m preprocess_time = time.time() - start_time\n\u001b[32m   1072\u001b[39m start_time = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MR/lang-sam/sam2/sam2_object_tracker.py:797\u001b[39m, in \u001b[36mSAM2ObjectTracker.preprocess_image\u001b[39m\u001b[34m(self, img, img_mean, img_std)\u001b[39m\n\u001b[32m    794\u001b[39m img /= img_std\n\u001b[32m    796\u001b[39m \u001b[38;5;66;03m# Move to device\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m img = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.float().unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "text_prompt = llama.get_response(input(\"What are you looking for? \"))\n",
    "\n",
    "first_frame = True\n",
    "with torch.inference_mode(), torch.autocast('cuda:0', dtype=torch.bfloat16):\n",
    "    while video_stream.isOpened():\n",
    "        ret, frame = video_stream.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        if first_frame:\n",
    "            image = Image.fromarray(img)\n",
    "            bbox = get_bbox([image], text_prompt, 0.3, 0.25)\n",
    "            xyxy = bbox[0][0]\n",
    "            bbox = [[xyxy[0], xyxy[1]], [xyxy[2], xyxy[3]]]\n",
    "            bbox = np.array(bbox, dtype=np.float32)\n",
    "            sam_out = sam.track_new_object(img=img,\n",
    "                                           box=bbox\n",
    "                                           )\n",
    "            \n",
    "            first_frame = False\n",
    "            \n",
    "        else:\n",
    "            sam_out = sam.track_all_objects(img=img)\n",
    "        \n",
    "        ret, frame = video_stream.read()\n",
    "        visualizer.add_frame(frame=frame, mask=sam_out['pred_masks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d37aa6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_stream.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abbb634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang sam",
   "language": "python",
   "name": "lang_sam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
