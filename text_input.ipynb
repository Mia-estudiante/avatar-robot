{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from models.gdino import GDINO\n",
    "from models.llama import Llama\n",
    "from visualizer import Visualizer\n",
    "\n",
    "from sam2.build_sam import build_sam2_object_tracker\n",
    "torch_dtype=torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb66ca244d0f0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set SAM2 Configuration\n",
    "NUM_OBJECTS = 1\n",
    "SAM_CHECKPOINT_FILEPATH = \"./checkpoints/sam2.1_hiera_tiny.pt\"\n",
    "SAM_CONFIG_FILEPATH = \"./configs/samurai/sam2.1_hiera_t.yaml\"\n",
    "# SAM_CONFIG_FILEPATH = \"./configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b3205",
   "metadata": {},
   "source": [
    "parameter를 이용해 hugging face로부터 모델을 불러올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d909a1eff131e1e1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sam = build_sam2_object_tracker(num_objects=NUM_OBJECTS,\n",
    "                                config_file=SAM_CONFIG_FILEPATH,\n",
    "                                ckpt_path=SAM_CHECKPOINT_FILEPATH,\n",
    "                                device=DEVICE,\n",
    "                                verbose=False\n",
    "                                )\n",
    "gdino = GDINO()\n",
    "gdino.build_model()\n",
    "\n",
    "llama = Llama()\n",
    "llama.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b73b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Video Stream\n",
    "# video_stream = cv2.VideoCapture(VIDEO_STREAM)\n",
    "video_stream = cv2.VideoCapture(0)\n",
    "\n",
    "video_height = int(video_stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "video_width = int(video_stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "# For real-time visualization\n",
    "visualizer = Visualizer(video_width=video_width, video_height=video_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(images_pil, texts_prompt, box_threshold, text_threshold):\n",
    "    gdino_results = gdino.predict(images_pil, [texts_prompt], box_threshold, text_threshold)\n",
    "    sam_boxes = []\n",
    "    sam_indices = []\n",
    "    for idx, result in enumerate(gdino_results):\n",
    "        result = {k: (v.cpu().numpy() if hasattr(v, \"numpy\") else v) for k, v in result.items()}\n",
    "        processed_result = {\n",
    "            **result,\n",
    "            \"masks\": [],\n",
    "            \"mask_scores\": [],\n",
    "        }\n",
    "\n",
    "        sam_boxes.append(processed_result[\"boxes\"])\n",
    "        sam_indices.append(idx)\n",
    "\n",
    "    return sam_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293c841",
   "metadata": {},
   "source": [
    "아래는 query의 예시입니다.   \n",
    "query = \"I am dehydrated\"   \n",
    "query = \"I am thirsty\"   \n",
    "query = \"I want to read\"   \n",
    "query = \"I am bored\"   \n",
    "query = \"I need a tool for writing\"   \n",
    "query = \"I have to write something down\"   \n",
    "query = \"I have to call him\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e8aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt = llama.get_response(input(\"What are you looking for? \"))\n",
    "\n",
    "first_frame = True\n",
    "with torch.inference_mode(), torch.autocast('cuda:0', dtype=torch.bfloat16):\n",
    "    while video_stream.isOpened():\n",
    "        ret, frame = video_stream.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        if first_frame:\n",
    "            image = Image.fromarray(img)\n",
    "            bbox = get_bbox([image], text_prompt, 0.3, 0.25)\n",
    "            xyxy = bbox[0][0]\n",
    "            bbox = [[xyxy[0], xyxy[1]], [xyxy[2], xyxy[3]]]\n",
    "            bbox = np.array(bbox, dtype=np.float32)\n",
    "            sam_out = sam.track_new_object(img=img,\n",
    "                                           box=bbox\n",
    "                                           )\n",
    "            \n",
    "            first_frame = False\n",
    "            \n",
    "        else:\n",
    "            sam_out = sam.track_all_objects(img=img)\n",
    "        \n",
    "        ret, frame = video_stream.read()\n",
    "        visualizer.add_frame(frame=frame, mask=sam_out['pred_masks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37aa6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_stream.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abbb634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang sam",
   "language": "python",
   "name": "lang_sam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
