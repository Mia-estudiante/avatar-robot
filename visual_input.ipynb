{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b1c46-9f5c-41c1-9101-85db8709ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2",
   "metadata": {},
   "source": [
    "# Run Segment Anything Model 2 on a live video stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from IPython import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "847fa5b7-a9ff-4e52-8b32-808160c72cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_optimize():\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    \n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86137627-dc94-4d05-be55-95869c0e54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e0779-751f-4224-9b04-ed0f0b406500",
   "metadata": {},
   "source": [
    "### Loading the SAM 2 camera predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1747ebab-8c86-4bc2-81ad-0a7836354da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_camera_predictor\n",
    "\n",
    "def get_predictor(model='tiny'):\n",
    "    if model=='tiny':\n",
    "        sam2_checkpoint = \"./checkpoints/sam2.1_hiera_tiny.pt\"\n",
    "        model_cfg = \"configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
    "    elif model=='small':\n",
    "        sam2_checkpoint = \"./checkpoints/sam2.1_hiera_small.pt\"\n",
    "        model_cfg = \"configs/sam2.1/sam2.1_hiera_s.yaml\"\n",
    "    elif model=='large':\n",
    "        sam2_checkpoint = \"./checkpoints/sam2.1_hiera_large.pt\"\n",
    "        model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "    return build_sam2_camera_predictor(model_cfg, sam2_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64f966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask_cv(mask, background, obj_id=None, random_color=False, alpha=0.6, pause=1000):\n",
    "    if random_color:\n",
    "        color = np.random.randint(0, 256, size=(3,), dtype=np.uint8)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        rgb_color = (np.array(cmap(cmap_idx)[:3]) * 255).astype(np.uint8)\n",
    "        color = tuple(rgb_color[::-1]) #RGB2BGR\n",
    "\n",
    "    color_mask = np.zeros_like(background, dtype=np.uint8)\n",
    "    for i in range(3):  # RGB\n",
    "        color_mask[:, :, i] = mask * color[i]\n",
    "\n",
    "    overlay = cv2.addWeighted(background, 1.0, color_mask, alpha, 0)\n",
    "\n",
    "    cv2.imshow(\"Masked Frame\", overlay)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a30cba",
   "metadata": {},
   "source": [
    "#### Step 1: Add a first click on the first frame\n",
    "mask input is excluded   \n",
    "default: by point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb71bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecting_obj(predictor, frame, points=None, labels=None,\n",
    "                  bbox=None, ann_frame_idx=0, ann_obj_id=(1)):\n",
    "    predictor.load_first_frame(frame)\n",
    "\n",
    "    if bbox is not None:\n",
    "        boxes = np.array(bbox, dtype=np.float32).reshape(1, 4)\n",
    "        points_arr = None\n",
    "        labels_arr = None\n",
    "    else:\n",
    "        points_arr = np.array(points if points is not None else [[310, 360]], dtype=np.float32)\n",
    "        labels_arr = np.array(labels if labels is not None else [1], dtype=np.int32)\n",
    "        boxes = None\n",
    "\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_prompt(\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=ann_obj_id,\n",
    "        points=points_arr,\n",
    "        labels=labels_arr,\n",
    "        bbox=boxes\n",
    "    )\n",
    "\n",
    "    mask = (out_mask_logits[0] > 0.0).cpu().numpy()\n",
    "    show_mask_cv(mask, frame, obj_id=out_obj_ids[0])\n",
    "\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b94c87ca-fd1a-4011-9609-e8be1cbe3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로로 저장된 비디오 입력 가능\n",
    "# cap = cv2.VideoCapture(\"videos/aquarium/aquarium.mp4\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "ret, frame = cap.read()\n",
    "width, height = frame.shape[:2][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdf57b99-112d-4feb-8e9c-2526310afa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/home/MR/lang-sam/sam2/sam2_camera_predictor.py:1118: UserWarning: /home/home/MR/lang-sam/sam2/_C.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKSs\n",
      "\n",
      "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/sam2/blob/main/INSTALL.md).\n",
      "  pred_masks_gpu = fill_holes_in_mask_scores(\n"
     ]
    }
   ],
   "source": [
    "predictor = get_predictor()\n",
    "\n",
    "## point(default)로 객체 선택\n",
    "# predictor = selecting_obj(predictor, frame)\n",
    "\n",
    "# bounding box로 객체 선택 (예시)\n",
    "predictor = selecting_obj(predictor, frame, bbox=[100, 100, 300, 450])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aeb04d-8cba-4f57-95da-6e5a1796003e",
   "metadata": {},
   "source": [
    "#### Step 2: track\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bd22b9-76e8-407d-a538-392ed78bd541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to track FPS and delayed time\n",
    "li = []\n",
    "def track_selected(predictor, cap, vis_gap=1, ann_frame_idx=0):\n",
    "    \n",
    "    delayed_time = 0\n",
    "    prev_time = 0\n",
    "    total_elapsed_time = 0\n",
    "    tracking = True\n",
    "    while tracking:\n",
    "    \n",
    "        ret, frame = cap.read()\n",
    "        prev_time = time.time()\n",
    "        ann_frame_idx += 1\n",
    "        if not ret:\n",
    "            break\n",
    "        width, height = frame.shape[:2][::-1]\n",
    "    \n",
    "        out_obj_ids, out_mask_logits = predictor.track(frame)\n",
    "    \n",
    "#         if ann_frame_idx % vis_gap == 0:\n",
    "#             show_mask_cv((out_mask_logits[0] > 0.0).cpu().numpy(), frame, obj_id=out_obj_ids[0])\n",
    "#             delayed_time = (time.time() - prev_time) * 1000\n",
    "#             if vis_gap <= 1:\n",
    "#                 total_elapsed_time += delayed_time\n",
    "#             else:\n",
    "#                 total_elapsed_time += delayed_time * vis_gap\n",
    "#             avg_delay = total_elapsed_time / ann_frame_idx\n",
    "#            print(f\"frame {ann_frame_idx} / delay {delayed_time:.2f}ms / avg_delay {avg_delay:.2f}ms\")\n",
    "        show_mask_cv((out_mask_logits[0] > 0.0).cpu().numpy(), frame, obj_id=out_obj_ids[0])\n",
    "        delayed_time = (time.time() - prev_time) * 1000\n",
    "        total_elapsed_time += delayed_time\n",
    "        avg_delay = total_elapsed_time / ann_frame_idx\n",
    "        if ann_frame_idx % 50 == 0:\n",
    "            li.append(f\"FPS: {1000/avg_delay}, delay: {avg_delay}ms\")\n",
    "            \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            tracking = False\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110883d8-6762-43fd-bb7a-c47bd8359000",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/home/MR/lang-sam/sam2/sam2_camera_predictor.py:837: UserWarning: /home/home/MR/lang-sam/sam2/_C.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKSs\n",
      "\n",
      "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/sam2/blob/main/INSTALL.md).\n",
      "  pred_masks_gpu = fill_holes_in_mask_scores(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrack_selected\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcap\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrack_selected\u001b[39m\u001b[34m(predictor, cap, vis_gap, ann_frame_idx)\u001b[39m\n\u001b[32m     15\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     16\u001b[39m         width, height = frame.shape[:\u001b[32m2\u001b[39m][::-\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         out_obj_ids, out_mask_logits = \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m#         if ann_frame_idx % vis_gap == 0:\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#             show_mask_cv((out_mask_logits[0] > 0.0).cpu().numpy(), frame, obj_id=out_obj_ids[0])\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#             delayed_time = (time.time() - prev_time) * 1000\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m \u001b[38;5;66;03m#             avg_delay = total_elapsed_time / ann_frame_idx\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m#            print(f\"frame {ann_frame_idx} / delay {delayed_time:.2f}ms / avg_delay {avg_delay:.2f}ms\")\u001b[39;00m\n\u001b[32m     29\u001b[39m         show_mask_cv((out_mask_logits[\u001b[32m0\u001b[39m] > \u001b[32m0.0\u001b[39m).cpu().numpy(), frame, obj_id=out_obj_ids[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang-sam2/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MR/lang-sam/sam2/sam2_camera_predictor.py:813\u001b[39m, in \u001b[36mSAM2CameraPredictor.track\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    804\u001b[39m \u001b[38;5;66;03m# Retrieve correct image features\u001b[39;00m\n\u001b[32m    805\u001b[39m (\n\u001b[32m    806\u001b[39m     _,\n\u001b[32m    807\u001b[39m     _,\n\u001b[32m   (...)\u001b[39m\u001b[32m    810\u001b[39m     feat_sizes,\n\u001b[32m    811\u001b[39m ) = \u001b[38;5;28mself\u001b[39m._get_feature(img, batch_size)\n\u001b[32m--> \u001b[39m\u001b[32m813\u001b[39m current_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_init_cond_frame\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurrent_vision_feats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_vision_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurrent_vision_pos_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_vision_pos_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeat_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeat_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpoint_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcondition_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_frames\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrack_in_reverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_mem_encoder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprev_sam_mask_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[38;5;66;03m# optionally offload the output to CPU memory to save GPU space\u001b[39;00m\n\u001b[32m    829\u001b[39m storage_device = \u001b[38;5;28mself\u001b[39m.condition_state[\u001b[33m\"\u001b[39m\u001b[33mstorage_device\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MR/lang-sam/sam2/modeling/sam2_base.py:833\u001b[39m, in \u001b[36mSAM2Base.track_step\u001b[39m\u001b[34m(self, frame_idx, is_init_cond_frame, current_vision_feats, current_vision_pos_embeds, feat_sizes, point_inputs, mask_inputs, output_dict, num_frames, track_in_reverse, run_mem_encoder, prev_sam_mask_logits)\u001b[39m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrack_step\u001b[39m(\n\u001b[32m    813\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    814\u001b[39m     frame_idx,\n\u001b[32m   (...)\u001b[39m\u001b[32m    831\u001b[39m     prev_sam_mask_logits=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    832\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m     current_out, sam_outputs, _, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_track_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_init_cond_frame\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_vision_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_vision_pos_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeat_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpoint_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrack_in_reverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprev_sam_mask_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    847\u001b[39m     (\n\u001b[32m    848\u001b[39m         _,\n\u001b[32m    849\u001b[39m         _,\n\u001b[32m   (...)\u001b[39m\u001b[32m    854\u001b[39m         object_score_logits,\n\u001b[32m    855\u001b[39m     ) = sam_outputs\n\u001b[32m    857\u001b[39m     current_out[\u001b[33m\"\u001b[39m\u001b[33mpred_masks\u001b[39m\u001b[33m\"\u001b[39m] = low_res_masks\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MR/lang-sam/sam2/modeling/sam2_base.py:759\u001b[39m, in \u001b[36mSAM2Base._track_step\u001b[39m\u001b[34m(self, frame_idx, is_init_cond_frame, current_vision_feats, current_vision_pos_embeds, feat_sizes, point_inputs, mask_inputs, output_dict, num_frames, track_in_reverse, prev_sam_mask_logits)\u001b[39m\n\u001b[32m    754\u001b[39m     sam_outputs = \u001b[38;5;28mself\u001b[39m._use_mask_as_output(\n\u001b[32m    755\u001b[39m         pix_feat, high_res_features, mask_inputs\n\u001b[32m    756\u001b[39m     )\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    758\u001b[39m     \u001b[38;5;66;03m# fused the visual feature with previous memory features in the memory bank\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m     pix_feat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_memory_conditioned_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_init_cond_frame\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_init_cond_frame\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_vision_feats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_vision_feats\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_vision_pos_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_vision_pos_embeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeat_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeat_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrack_in_reverse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrack_in_reverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    769\u001b[39m     \u001b[38;5;66;03m# apply SAM-style segmentation head\u001b[39;00m\n\u001b[32m    770\u001b[39m     \u001b[38;5;66;03m# here we might feed previously predicted low-res SAM mask logits into the SAM mask decoder,\u001b[39;00m\n\u001b[32m    771\u001b[39m     \u001b[38;5;66;03m# e.g. in demo where such logits come from earlier interaction instead of correction sampling\u001b[39;00m\n\u001b[32m    772\u001b[39m     \u001b[38;5;66;03m# (in this case, any `mask_inputs` shouldn't reach here as they are sent to _use_mask_as_output instead)\u001b[39;00m\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prev_sam_mask_logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MR/lang-sam/sam2/modeling/sam2_base.py:631\u001b[39m, in \u001b[36mSAM2Base._prepare_memory_conditioned_features\u001b[39m\u001b[34m(self, frame_idx, is_init_cond_frame, current_vision_feats, current_vision_pos_embeds, feat_sizes, output_dict, num_frames, track_in_reverse)\u001b[39m\n\u001b[32m    629\u001b[39m t_diff_max = max_obj_ptrs_in_encoder - \u001b[32m1\u001b[39m\n\u001b[32m    630\u001b[39m tpos_dim = C \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proj_tpos_enc_in_obj_ptrs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mem_dim\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m obj_pos = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m obj_pos = get_1d_sine_pe(obj_pos / t_diff_max, dim=tpos_dim)\n\u001b[32m    633\u001b[39m obj_pos = \u001b[38;5;28mself\u001b[39m.obj_ptr_tpos_proj(obj_pos)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "track_selected(predictor, cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "627d54e2-a2c1-47bc-b76c-76ee8274e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d664b935-02ae-487b-84b8-e329b2315a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FPS: 17.839920057090414, delay: 56.054062843322754ms',\n",
       " 'FPS: 17.929233818051557, delay: 55.774831771850586ms',\n",
       " 'FPS: 17.92559868613261, delay: 55.786142349243164ms',\n",
       " 'FPS: 17.885920956142563, delay: 55.90989708900452ms',\n",
       " 'FPS: 17.74385223889356, delay: 56.357547760009766ms',\n",
       " 'FPS: 17.78134339801432, delay: 56.238720417022705ms',\n",
       " 'FPS: 17.804898244419217, delay: 56.16431985582624ms',\n",
       " 'FPS: 17.825416440757365, delay: 56.09967112541199ms',\n",
       " 'FPS: 17.846419248295565, delay: 56.03364944458008ms',\n",
       " 'FPS: 17.828174325450586, delay: 56.09099292755127ms',\n",
       " 'FPS: 17.800470410103237, delay: 56.17829062721946ms',\n",
       " 'FPS: 17.776226037872103, delay: 56.2549102306366ms',\n",
       " 'FPS: 17.749103727414674, delay: 56.34087305802565ms']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49513e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang sam",
   "language": "python",
   "name": "lang_sam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
